{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import findspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = \"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Linear Regression Model\") \\\n",
    "    .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some tests\n",
    "TOTAL = 1000\n",
    "dots = sc.parallelize([2.0 * np.random.random(2) - 1.0 for i in range(TOTAL)]).cache()\n",
    "print(\"Number of random points:\", dots.count())\n",
    "\n",
    "stats = dots.stats()\n",
    "print('Mean: ', stats.mean())\n",
    "print('stdev: ', stats.stdev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some more tests\n",
    "rdd1 = sc.parallelize([('a', 7), ('a', 2), ('b', 2)])\n",
    "rdd2 = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"q\", \"r\"])])\n",
    "rdd3 = sc.parallelize(range(10))\n",
    "\n",
    "rdd2.flatMapValues(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Boston housing data - method 1\n",
    "housing = spark.read.csv(\"D:/HP Pavilion D drive/Mit/Udemy/Aurélien Géron_Hands-on ML Scikit-Learn-DL TensorFlow - dwnld from GitHub/datasets/housing/housing.csv\", header = True, mode = \"DROPMALFORMED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Boston housing data - method 2\n",
    "housing = spark.read.format(\"CSV\").option(\"header\", \"true\").load(\"D:/HP Pavilion D drive/Mit/Udemy/Aurélien Géron_Hands-on ML Scikit-Learn-DL TensorFlow - dwnld from GitHub/datasets/housing/housing.csv\")\n",
    "type(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print DataFrame object\n",
    "housing.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change numeric string variables to float\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "housing = housing.withColumn(\"longitude\", housing[\"longitude\"].cast(FloatType())) \\\n",
    "    .withColumn(\"latitude\", housing[\"latitude\"].cast(FloatType())) \\\n",
    "    .withColumn(\"housing_median_age\", housing[\"housing_median_age\"].cast(FloatType())) \\\n",
    "    .withColumn(\"total_rooms\", housing[\"total_rooms\"].cast(FloatType())) \\\n",
    "    .withColumn(\"total_bedrooms\", housing[\"total_bedrooms\"].cast(FloatType())) \\\n",
    "    .withColumn(\"population\", housing[\"population\"].cast(FloatType())) \\\n",
    "    .withColumn(\"households\", housing[\"households\"].cast(FloatType())) \\\n",
    "    .withColumn(\"median_income\", housing[\"median_income\"].cast(FloatType())) \\\n",
    "    .withColumn(\"median_house_value\", housing[\"median_house_value\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively,\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names:\n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "# Assign all column names to columns\n",
    "# columns = ['households', 'housing_median_age', 'latitude', 'longitude', 'median_house_value', 'median_income', 'population', 'total_bedrooms', 'total_rooms']\n",
    "columns = housing.columns\n",
    "\n",
    "# Convert the df columns to FloatType()\n",
    "housing = convertColumn(housing, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing.select('population', 'total_bedrooms').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing.groupBy(\"housing_median_age\").count().sort(\"housing_median_age\", ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjust the values of median_house_value by dividing by 1 lakh\n",
    "housing = housing.withColumn(\"median_house_value\", housing[\"median_house_value\"] / 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add new columns\n",
    "housing = housing.withColumn(\"roomsPerHousehold\", housing[\"total_rooms\"] / housing[\"households\"]) \\\n",
    "    .withColumn(\"populationPerHousehold\", housing[\"population\"] / housing[\"households\"])\n",
    "\n",
    "# Inspect the result\n",
    "housing.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "housing = housing.select(\"total_bedrooms\", \"population\", \"households\", \"median_income\", \"roomsPerHousehold\", \"populationPerHousehold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardisation of variables\n",
    "# Import DenseVector\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the input_data\n",
    "input_data = housing.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace housing with the new DataFrame\n",
    "housing = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialise the standardScaler\n",
    "standardScaler = StandardScaler(inputCol = \"features\", outputCol = \"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(housing)\n",
    "\n",
    "# Transform the data in housing with the scaler\n",
    "scaled_housing = scaler.transform(housing)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_housing.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_housing.randomSplit([.8, .2], seed = 1234)\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
