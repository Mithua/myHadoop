{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark data frames from CSV files: handling headers & column types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start pyspark with IPython notebook with ``IPYTHON_OPTS=\"notebook\" $SPARK_HOME/bin/pyspark`` from the command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install findspark py4j pyspark tflearn keras theano tensorflow-gpu\n",
    "\n",
    "# wget 'https://www.nodalpoint.com/wp-content/uploads/2015/05/nyctaxisub.rar'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxiFile = sc.textFile(\"file:///home/carnd/nyctaxisub.csv\")\n",
    "taxiFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxiFile.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's isolate the header, in order to eventually use it to get the field names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = taxiFile.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get rid of these double quotes around the field names, and then use the header to build the fields for our schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schemaString = header.replace('\"','')\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split(',')]\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many elements there are in the header (i.e. how many data columns)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's modify the fields which should not be of type ``String``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields[2].dataType = TimestampType()\n",
    "fields[3].dataType = FloatType()\n",
    "fields[4].dataType = FloatType()\n",
    "fields[7].dataType = IntegerType()\n",
    "fields[8].dataType = TimestampType()\n",
    "fields[9].dataType = FloatType()\n",
    "fields[10].dataType = FloatType()\n",
    "fields[11].dataType = IntegerType()\n",
    "fields[13].dataType = FloatType()\n",
    "fields[14].dataType = IntegerType()\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get rid of the leading underscores in the first two field names (``_id`` and ``_rev``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields[0].name = 'id'\n",
    "fields[1].name = 'rev'\n",
    "fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are satisfied with the data types, we can construct our schema, which we will use later below for building the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType(fields)\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate the header and drop it off the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxiHeader = taxiFile.filter(lambda l: \"_id\" in l)\n",
    "taxiHeader.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxiNoHeader = taxiFile.subtract(taxiHeader)\n",
    "taxiNoHeader.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with 249,999 rows, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before parsing the data, we have to import the necessary Python modules to handle ``datetimes``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import *\n",
    "from dateutil.parser import parse\n",
    "# test it:\n",
    "parse(\"2013-02-09 18:16:10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for our first attempt to parse the data with the correct types. We build a temporary RDD for this purpose - ``taxi_temp``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_temp = taxiNoHeader.map(lambda k: k.split(\",\")).map(lambda p: (p[0], p[1], parse(p[2].strip('\"')), float(p[3]), float(p[4]) , p[5], p[6] , int(p[7]), parse(p[8].strip('\"')), float(p[9]), float(p[10]), int(p[11]), p[12], float(p[13]), int(p[14]), p[15] ))\n",
    "taxi_temp.top(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's build our dataframe, using the ``taxi_temp`` RDD just produced and the ``schema`` variable computed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df = sqlContext.createDataFrame(taxi_temp, schema)\n",
    "taxi_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we still have quotes-within-quotes in our ``StringType`` variables. We make a second attempt, this time using Spark's ``rdd.toDF()`` method, in order to build the dataframe directly from ``taxiNoHeader`` RDD, without invoking the temporary ``taxi_temp`` RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df = taxiNoHeader.map(lambda k: k.split(\",\")).map(lambda p: (p[0].strip('\"'), p[1].strip('\"'), parse(p[2].strip('\"')), float(p[3]), float(p[4]) , p[5].strip('\"'), p[6].strip('\"') , int(p[7]), parse(p[8].strip('\"')), float(p[9]), float(p[10]), int(p[11]), p[12].strip('\"'), float(p[13]), int(p[14]), p[15].strip('\"')) ).toDF(schema)\n",
    "taxi_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run some simple pandas-like queries. How many records per vendor are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df.groupBy(\"vendor_id\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we have missing values in the field ``store_and_fwd_flag``. How many are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df.filter(taxi_df.store_and_fwd_flag == '').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the number of missing values is dangerously close to the number of ``VTS`` vendor records. Is this a coincidence, or vendor ``VTS`` indeed tends not to log the subject variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df.filter(taxi_df.store_and_fwd_flag == '' and taxi_df.vendor_id == 'VTS').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we have a finding! Indeed, all records coming from ``VTS`` vendor have missing value in the subject field..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``dtypes`` and ``printSchema()`` methods can be used to get information about the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the SQL equivalent of the above pandas-like queries. First, we have to register the dataframe as a named temporary table, let's say ``taxi``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df.registerTempTable(\"taxi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT vendor_id, COUNT(*) FROM taxi GROUP BY vendor_id \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT COUNT(*) FROM taxi WHERE store_and_fwd_flag = '' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT COUNT(*) FROM taxi WHERE vendor_id = 'VTS' AND store_and_fwd_flag = '' \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, unlike standard SQL, table and column names are case sensitive, i.e. ``TAXI`` or ``vendor_ID`` in the queries will produce an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change some column names to shorter versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.withColumnRenamed('dropoff_longitude', 'dropoff_long').withColumnRenamed('dropoff_latitude', 'dropoff_lat').withColumnRenamed('pickup_latitude', 'pickup_lat').withColumnRenamed('pickup_longitude', 'pickup_long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's make a row selection and store the results to a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "taxi_CMT = taxi_df.filter(\"vendor_id = 'CMT' and store_and_fwd_flag != '' \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = taxi_CMT.head()\n",
    "b = taxi_CMT.tail()\n",
    "print(a)\n",
    "print(b)\n",
    "print(taxi_CMT.columns)\n",
    "print(taxi_CMT.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
